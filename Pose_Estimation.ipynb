{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3yV2MWLhm3vb4PTko6e+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NethmiJayasinghe/Pose-Estimation-for-an-RGB-input/blob/main/Pose_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYEOoNKBAfAn"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pylab as plt\n",
        "from skimage.transform import resize\n",
        "from skimage import color\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as fn\n",
        "import math\n",
        "import joblib\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Deep Layer Aggregation\n",
        "\n",
        "class CentDla(nn.Module):\n",
        "    def __init__(self, batch_size, num_Input_channels):\n",
        "      super().__init__()\n",
        "      self.c1_0 = nn.Conv2d(num_Input_channels, 16, 1, stride = 1)\n",
        "      self.c1_1 = nn.Conv2d(16, 16, 5, stride = 2, padding= 2)\n",
        "      self.c1_2 = nn.Conv2d(16, 16, 5, stride = 2, padding= 2)\n",
        "\n",
        "      #stage1\n",
        "      self.s1_c1 = nn.Conv2d(16, 16, 1, stride = 1)\n",
        "      self.s1_c2 = nn.Conv2d(16, 16, 1, stride = 1)\n",
        "      self.normalize_1 = nn.BatchNorm2d(16)\n",
        "\n",
        "      #stage2\n",
        "      self.s2_c1 = nn.Conv2d(16, 32, 5, stride = 2, padding= 2)\n",
        "      self.s2_c2 = nn.Conv2d(32, 32, 1, stride = 1)\n",
        "\n",
        "      #self.aggregate_2 = nn.Conv2d(64,64,1,stride=1)\n",
        "      self.normalize_2 = nn.BatchNorm2d(32)\n",
        "      self.s2_c3 = nn.Conv2d(32,32,1,stride = 1)\n",
        "      self.s2_c4 = nn.Conv2d(32,32,1,stride = 1)\n",
        "\n",
        "      self.aggregate_3 = nn.Conv2d(16,32,5,stride=2, padding =2)\n",
        "      self.normalize_3 = nn.BatchNorm2d(32)\n",
        "      self.u1 = nn.ConvTranspose2d(32,64,2, stride = 2, dilation = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.rrelu(self.c1_0(x))                           #[16, 512, 512]\n",
        "        #x = self.p1_1(x)\n",
        "        x = F.rrelu(self.c1_1(x))                           #[16, 256, 256]\n",
        "        x = F.rrelu(self.c1_2(x))                           #[16, 256, 256]\n",
        "\n",
        "        #stage1\n",
        "        x = F.rrelu(self.s1_c1(x))                          #[16, 128, 128]\n",
        "        aggregate_layer_1 = x                               #[16, 128, 128]\n",
        "        x = F.rrelu(self.s1_c2(x))                          #[16, 128, 128]\n",
        "        #x = torch.squeeze(F.rrelu(self.normalize_1(torch.unsqueeze(self.aggregate_1(torch.cat((aggregate_layer_1,x),0)),0))))    #[32, 128, 128]\n",
        "        x = torch.squeeze( F.rrelu( self.normalize_1(aggregate_layer_1+x) ) )    #[32, 128, 128]\n",
        "        ida_1 = x\n",
        "\n",
        "        #stage2\n",
        "        x = F.rrelu(self.s2_c1(x))                          #[32, 64, 64]\n",
        "        x = F.rrelu(self.s2_c2(x))                          #[32, 64, 64]\n",
        "        aggregate_layer_2 = x  \n",
        "        x = torch.squeeze(F.rrelu(self.normalize_2((aggregate_layer_2+x))))    #[32, 64, 64]\n",
        "        aggregate_layer_2 = x \n",
        "        x = F.rrelu(self.s2_c3(x))    #[32, 64, 64]\n",
        "        temp1=x                        \n",
        "        x = F.rrelu(self.s2_c4(x))      #[32, 64, 64]\n",
        "        x = aggregate_layer_2+temp1+x   \n",
        "        x = F.rrelu(torch.squeeze( self.normalize_3(self.aggregate_3(ida_1) ) + x))   #[32, 64, 64] \n",
        "        x = self.u1(x)              \n",
        "        return x"
      ],
      "metadata": {
        "id": "rqJbBV8GBo7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement ConvGRU\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, dtype):\n",
        "        \"\"\"\n",
        "        Initialize the ConvLSTM cell\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvGRUCell, self).__init__()\n",
        "        self.height, self.width = input_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bias = bias\n",
        "        self.dtype = dtype\n",
        "\n",
        "        self.conv_gates = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
        "                                    out_channels=2*self.hidden_dim,  # for update_gate,reset_gate respectively\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    padding=self.padding,\n",
        "                                    bias=self.bias)\n",
        "\n",
        "        self.conv_can = nn.Conv2d(in_channels=input_dim+hidden_dim,\n",
        "                              out_channels=self.hidden_dim, # for candidate neural memory\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).type(self.dtype))\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor, h_cur):\n",
        "        \"\"\"\n",
        "        :param self:\n",
        "        :param input_tensor: (b, c, h, w)\n",
        "            input is actually the target_model\n",
        "        :param h_cur: (b, c_hidden, h, w)\n",
        "            current hidden and cell states respectively\n",
        "        :return: h_next,\n",
        "            next hidden state\n",
        "        \"\"\"\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
        "        combined_conv = self.conv_gates(combined)\n",
        "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        reset_gate = torch.sigmoid(gamma)\n",
        "        update_gate = torch.sigmoid(beta)\n",
        "\n",
        "        combined = torch.cat([input_tensor, reset_gate*h_cur], dim=1)\n",
        "        cc_cnm = self.conv_can(combined)\n",
        "        cnm = torch.tanh(cc_cnm)\n",
        "\n",
        "        h_next = (1 - update_gate) * h_cur + update_gate * cnm\n",
        "        return h_next\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvGRU(nn.Module):\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 dtype, batch_first=False, bias=True, return_all_layers=False):\n",
        "\n",
        "        \"\"\"\n",
        "        :param input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        :param input_dim: int e.g. 256\n",
        "            Number of channels of input tensor.\n",
        "        :param hidden_dim: int e.g. 1024\n",
        "            Number of channels of hidden state.\n",
        "        :param kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        :param num_layers: int\n",
        "            Number of ConvLSTM layers\n",
        "        :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "            Whether or not to use cuda.\n",
        "        :param alexnet_path: str\n",
        "            pretrained alexnet parameters\n",
        "        :param batch_first: bool\n",
        "            if the first position of array is batch or not\n",
        "        :param bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        :param return_all_layers: bool\n",
        "            if return hidden and cell states for all layers\n",
        "        \"\"\"\n",
        "        super(ConvGRU, self).__init__()\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dtype = dtype\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = input_dim if i == 0 else hidden_dim[i - 1]\n",
        "            cell_list.append(ConvGRUCell(input_size=(self.height, self.width),\n",
        "                                         input_dim=cur_input_dim,\n",
        "                                         hidden_dim=self.hidden_dim[i],\n",
        "                                         kernel_size=self.kernel_size[i],\n",
        "                                         bias=self.bias,\n",
        "                                         dtype=self.dtype))\n",
        "\n",
        "        # convert python list to pytorch module\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "\n",
        "        \"\"\"\n",
        "        :param input_tensor: (b, t, c, h, w) or (t,b,c,h,w) depends on if batch first or not\n",
        "            extracted features from alexnet\n",
        "        :param hidden_state:\n",
        "        :return: layer_output_list, last_state_list\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list   = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            h = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "\n",
        "                # input current hidden and cell state then compute the next hidden and cell state through ConvLSTMCell forward function\n",
        "\n",
        "                h = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], # (b,t,c,h,w)\n",
        "                                              h_cur=h)\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list   = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param\n",
        "\n"
      ],
      "metadata": {
        "id": "nTLAovGGEN58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating neural network\n",
        "\n",
        "class Pose_estimation_network(nn.Module):\n",
        "    def __init__(self, batch_size,input_ch):\n",
        "      super().__init__()\n",
        "      self.rescale = torchvision.transforms.Resize((512,512))\n",
        "      self.DLA = CentDla(batch_size,input_ch)\n",
        "      use_gpu = torch.cuda.is_available()\n",
        "      if use_gpu:\n",
        "        dtype = torch.cuda.FloatTensor # computation in GPU\n",
        "      else:\n",
        "        dtype = torch.FloatTensor\n",
        "      self.GRU_1 = ConvGRU(input_size=(128,128),input_dim=64,hidden_dim=[64,64],kernel_size=(3,3),num_layers=2,dtype=dtype,batch_first=True,bias = True,return_all_layers = False)\n",
        "      self.GRU_2 = ConvGRU(input_size=(128,128),input_dim=64,hidden_dim=[64,64],kernel_size=(3,3),num_layers=2,dtype=dtype,batch_first=True,bias = True,return_all_layers = False)\n",
        "      self.GRU_3 = ConvGRU(input_size=(128,128),input_dim=64,hidden_dim=[64,64],kernel_size=(3,3),num_layers=2,dtype=dtype,batch_first=True,bias = True,return_all_layers = False)\n",
        "\n",
        "    \n",
        "      self.Conv_branch_1 = nn.Conv2d(64,256,3,stride = 1,padding =1)\n",
        "      self.Conv_branch_2 = nn.Conv2d(64,256,3,stride = 1,padding =1)\n",
        "      self.Conv_branch_3 = nn.Conv2d(64,256,3,stride = 1,padding =1)\n",
        "\n",
        "      self.Conv_Object_detection_branch_1 = nn.Conv2d(256,1,1,stride = 1)\n",
        "      self.Conv_Object_detection_branch_2 = nn.Conv2d(256,2,1,stride = 1)\n",
        "      self.Conv_Object_detection_branch_3 = nn.Conv2d(256,2,1,stride = 1)\n",
        "      self.Conv_Keypoint_detection_branch_1 = nn.Conv2d(256,8,1,stride = 1)\n",
        "      self.Conv_Keypoint_detection_branch_2 = nn.Conv2d(256,16,1,stride = 1)\n",
        "      self.Conv_Keypoint_detection_branch_3 = nn.Conv2d(256,16,1,stride = 1)\n",
        "      self.Conv_Cuboid_dimensions_branch = nn.Conv2d(256,3,1,stride = 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.rescale(x)\n",
        "      x = self.DLA(x)\n",
        "      Inp = torch.unsqueeze(x,0)\n",
        "      x = self.GRU_1( Inp )\n",
        "      branch_1 = x[0][0][0]\n",
        "      x = self.GRU_2(x[0][0])\n",
        "      branch_2 = x[0][0][0]\n",
        "      x = self.GRU_3(x[0][0])\n",
        "      branch_3 = x[0][0][0]\n",
        "\n",
        "      branch_1 = self.Conv_branch_1(branch_1)\n",
        "      branch_2 = self.Conv_branch_1(branch_2)\n",
        "      branch_3 = self.Conv_branch_1(branch_3)\n",
        "\n",
        "      object_center_heatmap = self.Conv_Object_detection_branch_1(branch_1)\n",
        "      subpixel_offset = self.Conv_Object_detection_branch_2(branch_1)\n",
        "      Bounding_box_sizes = self.Conv_Object_detection_branch_3(branch_1)\n",
        "\n",
        "      keypoint_heatmaps =  self.Conv_Keypoint_detection_branch_1(branch_2)\n",
        "      sub_pixel_offsets = self.Conv_Keypoint_detection_branch_2(branch_2)\n",
        "      keypoint_displacements = self.Conv_Keypoint_detection_branch_3(branch_2)\n",
        "\n",
        "      relative_cuboid_dimensions = self.Conv_Cuboid_dimensions_branch(branch_3)\n",
        "\n",
        "      return object_center_heatmap,subpixel_offset,Bounding_box_sizes,keypoint_heatmaps,sub_pixel_offsets,keypoint_displacements,relative_cuboid_dimensions\n",
        "\n",
        "# dummy = torch.rand(8,3,712,680)\n",
        "# model = Pose_estimation_network(8,3)\n",
        "# out1,out2,out3,out4,out5,out6,out7 = model(dummy)\n",
        "# #out1 = model(dummy)\n",
        "# print(out1.shape)\n",
        "# X = dummy\n",
        "# Y = torch.rand(1,128,128)\n",
        "\n"
      ],
      "metadata": {
        "id": "5UEHxOIknFw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from ..utils import _log_api_usage_once\n",
        "\n",
        "def sigmoid_focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 2,\n",
        "    gamma: float = 4,\n",
        "    reduction: str = \"mean\",):\n",
        "\n",
        "    \"\"\"\n",
        "    Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py .\n",
        "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples or -1 for ignore. Default = 0.25\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "\n",
        "    # if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
        "    #     _log_api_usage_once(sigmoid_focal_loss)\n",
        "    p = torch.sigmoid(inputs)\n",
        "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "HcLfQAsGnpec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = torch.rand(10,8,3,712,680)"
      ],
      "metadata": {
        "id": "3nNcB7xcnzni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y1 = torch.rand(10,8,1,128,128)\n",
        "Y2 = torch.rand(10,8,2,128,128)\n",
        "Y3 = torch.rand(10,8,2,128,128)\n",
        "Y4 = torch.rand(10,8,8,128,128)\n",
        "Y5 = torch.rand(10,8,16,128,128)\n",
        "Y6 = torch.rand(10,8,16,128,128)\n",
        "Y7 = torch.rand(10,8,3,128,128)"
      ],
      "metadata": {
        "id": "amlLvhx_n5hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pose_estimation_network(8,3)"
      ],
      "metadata": {
        "id": "CR63S7_Yn-ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "loss_vector=[]\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00007)"
      ],
      "metadata": {
        "id": "PfRhkzkDoD6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0uiWkwDrDx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}